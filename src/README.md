# Rust Web Crawler - Core Library

A high-performance, feature-rich web crawling library built with modern Rust architecture. This library implements a sophisticated web crawler with advanced content processing, intelligent rate limiting, and comprehensive session management.

## ğŸ—ï¸ Architecture Overview

This library follows a **feature-based architecture** organized by functionality rather than technical implementation, adhering to 4 core design principles:

1. **Modules by features/logic** with sub-modules for sub-features
2. **Building blocks assembled** into unified modules
3. **One level down organization** (feature/sub-feature)
4. **Logical cohesion** over technical separation

```
src/
â”œâ”€â”€ ğŸš€ bin/              # Entry Points & Executables
â”œâ”€â”€ âš™ï¸ config/           # Configuration Management
â”œâ”€â”€ ğŸ¯ core/             # Core Types & Utilities
â”œâ”€â”€ ğŸ•·ï¸ crawler/          # Main Crawling Engine
â”œâ”€â”€ ğŸ“Š logging/          # Unified Logging System
â”œâ”€â”€ ğŸŒ network/          # Network Layer Components
â”œâ”€â”€ ğŸ”„ processing/       # Content Processing & Analysis
â”œâ”€â”€ ğŸ“‹ queue/            # Task Queue Management
â”œâ”€â”€ ğŸª session/          # Session Management & Orchestration
â”œâ”€â”€ ğŸ’¾ storage/          # Data Persistence
â””â”€â”€ ğŸ“š lib.rs            # Library Root & Re-exports
```

## ğŸ“ Detailed Module Structure

### ğŸš€ **Binary Targets** (`bin/`)

**Purpose**: Executable entry points for the crawler application.

```
bin/
â””â”€â”€ main.rs             # Primary CLI application entry point
```

**Features**:

- Command-line interface for standalone crawling
- Configuration file support
- Interactive session management

### âš™ï¸ **Configuration** (`config/`)

**Purpose**: Centralized configuration management with environment-specific settings.

```
config/
â”œâ”€â”€ mod.rs              # Configuration module orchestration
â”œâ”€â”€ crawler.rs          # Core crawler configuration types
â”œâ”€â”€ environment.rs      # Environment-specific settings
â””â”€â”€ presets.rs          # Predefined configuration presets
```

**Key Components**:

- `WebCrawlerConfig` - Main crawler configuration
- `EnvironmentConfig` - Environment variables and settings
- `LatinWordFilter` - Language filtering configuration
- Configuration presets for common use cases

### ğŸ¯ **Core** (`core/`)

**Purpose**: Fundamental types, traits, and utilities used throughout the library.

```
core/
â”œâ”€â”€ mod.rs              # Core module orchestration
â”œâ”€â”€ error.rs            # Error types and handling
â”œâ”€â”€ traits.rs           # Core traits and interfaces
â”œâ”€â”€ types.rs            # Fundamental data types
â””â”€â”€ utils.rs            # Utility functions and helpers
```

**Key Types**:

- `CrawlTask` - Individual crawl task representation
- `CrawlResult` - Task execution results
- `CrawlError` - Comprehensive error handling
- `TaskStatus` & `TaskPriority` - Task management types

### ğŸ•·ï¸ **Crawler** (`crawler/`)

**Purpose**: Main crawling engine that orchestrates the entire crawling process.

```
crawler/
â”œâ”€â”€ mod.rs              # Crawler module orchestration
â””â”€â”€ engine.rs           # Core crawling engine implementation
```

**Features**:

- `WebCrawler` - Main crawler implementation
- Asynchronous crawling with concurrent task processing
- Intelligent depth management and URL discovery
- Integration with all other subsystems

### ğŸ“Š **Logging** (`logging/`)

**Purpose**: Unified logging system with structured events and configurable output.

```
logging/
â”œâ”€â”€ mod.rs              # Logging module orchestration
â”œâ”€â”€ config.rs           # Logging configuration types
â”œâ”€â”€ events.rs           # Structured event definitions
â””â”€â”€ formatter.rs        # Custom log formatters
```

**Features**:

- Structured logging with tracing support
- Event-based logging for better observability
- Configurable log levels and formats
- Performance and error tracking

### ğŸŒ **Network** (`network/`)

**Purpose**: Network layer handling HTTP requests, DNS resolution, and rate limiting.

```
network/
â”œâ”€â”€ mod.rs              # Network module orchestration
â”œâ”€â”€ client.rs           # HTTP client management
â”œâ”€â”€ dns.rs              # DNS resolution and caching
â”œâ”€â”€ rate_limit.rs       # Intelligent rate limiting
â””â”€â”€ robots.rs           # robots.txt compliance
```

**Features**:

- `HttpClientManager` - Connection pooling and management
- `DnsResolver` - Cached DNS resolution
- `RateLimiter` - Domain-specific rate limiting
- `RobotsChecker` - robots.txt parsing and compliance

### ğŸ”„ **Processing** (`processing/`)

**Purpose**: Advanced content processing and analysis with multiple specialized sub-modules.

```
processing/
â”œâ”€â”€ mod.rs              # Processing module orchestration
â”œâ”€â”€ content.rs          # Core content processing
â”œâ”€â”€ discovery.rs        # Link discovery and extraction
â”œâ”€â”€ language.rs         # Language detection and filtering
â”œâ”€â”€ cleaning/           # Content cleaning and sanitization
â”‚   â”œâ”€â”€ mod.rs          # Cleaning module orchestration
â”‚   â”œâ”€â”€ cleaner.rs      # Main cleaning implementation
â”‚   â”œâ”€â”€ config.rs       # Cleaning configuration
â”‚   â””â”€â”€ rules.rs        # Cleaning rules and patterns
â”œâ”€â”€ extensive/          # Extensive link processing
â”‚   â”œâ”€â”€ mod.rs          # Extensive processing orchestration
â”‚   â”œâ”€â”€ config.rs       # Extensive processing configuration
â”‚   â”œâ”€â”€ link_processor.rs # Advanced link processing
â”‚   â””â”€â”€ queue_manager.rs # Processing queue management
â””â”€â”€ keyword/            # Keyword extraction and matching
    â”œâ”€â”€ mod.rs          # Keyword module orchestration
    â”œâ”€â”€ config.rs       # Keyword processing configuration
    â”œâ”€â”€ extractor.rs    # Keyword extraction algorithms
    â””â”€â”€ matcher.rs      # Keyword matching and scoring
```

**Sub-Module Features**:

#### **Cleaning** (`processing/cleaning/`)

- HTML sanitization and content cleaning
- Configurable cleaning rules and patterns
- Text normalization and formatting

#### **Extensive** (`processing/extensive/`)

- Advanced link discovery algorithms
- Intelligent crawl queue management
- Link prioritization and filtering

#### **Keyword** (`processing/keyword/`)

- Sophisticated keyword extraction
- Relevance scoring and matching
- Configurable keyword filtering strategies

### ğŸ“‹ **Queue** (`queue/`)

**Purpose**: Sophisticated task queue management with caching and priority handling.

```
queue/
â”œâ”€â”€ mod.rs              # Queue module orchestration
â”œâ”€â”€ cache.rs            # Queue caching mechanisms
â””â”€â”€ task_queue.rs       # Priority task queue implementation
```

**Features**:

- Priority-based task scheduling
- Duplicate URL detection and filtering
- Queue persistence and recovery
- Performance optimized caching

### ğŸª **Session** (`session/`)

**Purpose**: High-level session management and result orchestration.

```
session/
â”œâ”€â”€ mod.rs              # Session module orchestration
â”œâ”€â”€ manager.rs          # Session lifecycle management
â”œâ”€â”€ result_collector.rs # Result aggregation and collection
â””â”€â”€ statistics.rs       # Session statistics and metrics
```

**Features**:

- `SessionManager` - Complete session lifecycle management
- Real-time statistics and progress tracking
- Result aggregation and storage coordination
- Session persistence and recovery

### ğŸ’¾ **Storage** (`storage/`)

**Purpose**: Data persistence and metrics storage with multiple output formats.

```
storage/
â”œâ”€â”€ mod.rs              # Storage module orchestration
â”œâ”€â”€ data.rs             # Data persistence implementation
â””â”€â”€ metrics.rs          # Metrics and analytics storage
```

**Features**:

- Multiple storage backends (JSON, CSV, custom)
- Real-time metrics collection and storage
- Configurable data retention policies
- Export and import functionality

## ğŸš€ Getting Started

### Prerequisites

- **Rust**: 1.70+ (Edition 2024)
- **Tokio**: Async runtime for concurrent operations

### Installation

```bash
# Add to your Cargo.toml
[dependencies]
rust_web_crawler = { path = "path/to/rust_web_crawler" }

# Or clone and build
git clone <repository>
cd rust_web_crawler
cargo build --release
```

### Quick Start

```rust
use rust_web_crawler::{WebCrawler, WebCrawlerConfig};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create configuration
    let config = WebCrawlerConfig::default();

    // Initialize crawler
    let crawler = WebCrawler::new(config, 10, 3)?;

    // Start crawling
    let url = url::Url::parse("https://example.com")?;
    let result = crawler.init_crawling(url).await?;

    println!("Crawled content: {:?}", result);
    Ok(())
}
```

### CLI Usage

```bash
# Run the CLI application
cargo run --bin main

# With configuration file
cargo run --bin main -- --config config.toml

# Help information
cargo run --bin main -- --help
```

## ğŸ“¦ Dependencies

### Core Dependencies

- **tokio 1.0** - Async runtime with full features
- **reqwest 0.11** - HTTP client with JSON and SOCKS support
- **scraper 0.13** - HTML parsing and CSS selector support
- **anyhow 1.0** - Error handling and context

### Content Processing

- **lol_html 1.2** - High-performance HTML parsing
- **whatlang 0.16** - Language detection
- **unicode-segmentation 1.10** - Text segmentation
- **regex 1.10** - Pattern matching

### Utilities

- **uuid 1.0** - Unique identifier generation
- **serde 1.0** - Serialization framework
- **url 2.4** - URL parsing and manipulation
- **bloom 0.3** - Bloom filter for duplicate detection

### Development

- **criterion 0.5** - Benchmarking framework
- **tempfile 3.8** - Temporary file management for testing

## ğŸ¯ Key Features

### âœ… **High Performance**

- Asynchronous concurrent crawling
- Intelligent connection pooling
- Memory-efficient processing
- Bloom filter duplicate detection

### âœ… **Advanced Content Processing**

- Multi-language content analysis
- Sophisticated keyword extraction
- Configurable content cleaning
- Link discovery algorithms

### âœ… **Intelligent Rate Limiting**

- Domain-specific rate limiting
- Adaptive delay algorithms
- robots.txt compliance
- Respectful crawling practices

### âœ… **Robust Error Handling**

- Comprehensive error types
- Automatic retry mechanisms
- Graceful failure handling
- Detailed error reporting

### âœ… **Flexible Configuration**

- Environment-based configuration
- Preset configurations for common use cases
- Runtime configuration updates
- Validation and error checking

### âœ… **Comprehensive Monitoring**

- Real-time progress tracking
- Detailed session statistics
- Performance metrics collection
- Structured logging and events

## ğŸ”§ Configuration

### Basic Configuration

```rust
use rust_web_crawler::WebCrawlerConfig;

let config = WebCrawlerConfig {
    max_total_urls: 1000,
    max_crawl_depth: 3,
    enable_extension_crawling: true,
    enable_keyword_filtering: true,
    target_words: vec!["rust".to_string(), "web".to_string()],
    ..WebCrawlerConfig::default()
};
```

### Environment Configuration

```bash
export RUST_LOG=info
export CRAWLER_MAX_DEPTH=5
export CRAWLER_RATE_LIMIT=1000
```

## ğŸ§ª Testing

```bash
# Run all tests
cargo test

# Run with output
cargo test -- --nocapture

# Run specific module tests
cargo test core::

# Run benchmarks
cargo bench
```

## ğŸ“ˆ Performance

### Benchmarks

The library includes comprehensive benchmarks for:

- Core crawling operations
- Content processing algorithms
- Queue operations
- Network performance

Run benchmarks with:

```bash
cargo bench
```

### Performance Characteristics

- **Throughput**: 1000+ pages/minute (depending on configuration)
- **Memory**: Efficient memory usage with streaming processing
- **Concurrency**: Configurable concurrent request limits
- **Caching**: Intelligent caching for DNS and duplicate detection

## ğŸ› ï¸ Contributing

1. Follow the feature-based architecture pattern
2. Ensure comprehensive error handling
3. Add appropriate tests for new functionality
4. Update documentation for new features
5. Run benchmarks for performance-critical changes

## ğŸ“š Documentation

- **API Documentation**: `cargo doc --open`
- **Examples**: See `examples/` directory
- **Architecture Guide**: This README and module documentation
- **Performance Guide**: Benchmark results and optimization tips

---

**Built with modern Rust architecture for high-performance web crawling and content analysis.**
